{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40915ae3",
   "metadata": {},
   "source": [
    "## Main file for training and saving an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf061b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d5d5b",
   "metadata": {},
   "source": [
    "### Import the agent and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8746a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from Environment import cryptoTrade\n",
    "\n",
    "from networks.Deep_RL_agents import DNN_agent, convert_to_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b65548",
   "metadata": {},
   "source": [
    "### Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd049278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, memory, model, target_model, done):\n",
    "    learning_rate = 0.7\n",
    "    discount_factor = 0.9\n",
    "    \n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    if len(memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "    \n",
    "    batch_size = 64\n",
    "    mini_batch_indexes = np.random.choice(np.arange(len(memory)), size=batch_size, replace=False)\n",
    "\n",
    "    current_states = [memory[i][0] for i in mini_batch_indexes]\n",
    "    current_qs_list = model.predict(convert_to_1d(current_states, single=False))\n",
    "    \n",
    "    new_current_states = [memory[i][4] for i in mini_batch_indexes]\n",
    "    future_qs_list = target_model.predict(convert_to_1d(new_current_states, single=False))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, i in enumerate(mini_batch_indexes):\n",
    "        (observation, action, actual_action, reward, new_observation, done) = memory[i]\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "\n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "    \n",
    "    X = convert_to_1d(X, single=False)\n",
    "    return X, Y\n",
    "    print(X)\n",
    "    model.fit(X, np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ff177",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887bd785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:08:45.111569: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-07 16:08:45.111700: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Create the env and the model\n",
    "training_data_path = \"data/training_2015_2021.df\"\n",
    "env = cryptoTrade(training_data_path, episode_size=720)\n",
    "\n",
    "model = DNN_agent(env.observation_space, env.action_space)\n",
    "target_model = DNN_agent(env.observation_space, env.action_space)\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb27e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch rewards (profit): 0.00e+00 after 0 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:08:56.279465: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-07 16:08:56.346149: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-07 16:08:56.397838: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch rewards (profit): -9.10e+00 after 1 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:09:13.571535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch rewards (profit): 9.34e+01 after 2 steps\n",
      "Total epoch rewards (profit): -1.24e+02 after 3 steps\n",
      "Total epoch rewards (profit): -1.24e+02 after 4 steps\n",
      "Total epoch rewards (profit): 4.26e+01 after 5 steps\n",
      "Total epoch rewards (profit): -2.93e+02 after 6 steps\n",
      "Total epoch rewards (profit): 2.82e+02 after 7 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 8 steps\n",
      "Total epoch rewards (profit): -6.20e+02 after 9 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 10 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 11 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 12 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 13 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 14 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 15 steps\n",
      "Total epoch rewards (profit): -2.47e+01 after 16 steps\n",
      "Total epoch rewards (profit): 2.54e+02 after 17 steps\n",
      "Total epoch rewards (profit): 4.45e+01 after 18 steps\n",
      "Total epoch rewards (profit): -6.05e+01 after 19 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 20 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 21 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 22 steps\n",
      "Total epoch rewards (profit): 7.14e+02 after 23 steps\n",
      "Total epoch rewards (profit): -6.75e+01 after 24 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 25 steps\n",
      "Total epoch rewards (profit): 2.51e+02 after 26 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 27 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 28 steps\n",
      "Total epoch rewards (profit): -5.26e+01 after 29 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 30 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 31 steps\n",
      "Total epoch rewards (profit): 3.32e+03 after 32 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 33 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 34 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 35 steps\n",
      "Total epoch rewards (profit): 1.13e+02 after 36 steps\n",
      "Total epoch rewards (profit): -6.25e+02 after 37 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 38 steps\n",
      "Total epoch rewards (profit): 2.31e+02 after 39 steps\n",
      "Total epoch rewards (profit): 4.27e+02 after 40 steps\n",
      "Total epoch rewards (profit): 3.34e+02 after 41 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 42 steps\n",
      "Total epoch rewards (profit): -1.30e+03 after 43 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 44 steps\n",
      "Total epoch rewards (profit): -2.86e+00 after 45 steps\n",
      "Total epoch rewards (profit): 1.72e+00 after 46 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 47 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 48 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 49 steps\n",
      "Total epoch rewards (profit): 6.35e+02 after 50 steps\n",
      "Total epoch rewards (profit): -9.44e+01 after 51 steps\n",
      "Total epoch rewards (profit): 4.36e+02 after 52 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 53 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 54 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 55 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 56 steps\n",
      "Total epoch rewards (profit): -5.42e+02 after 57 steps\n",
      "Total epoch rewards (profit): -2.19e+03 after 58 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 59 steps\n",
      "Total epoch rewards (profit): 2.31e+02 after 60 steps\n",
      "Total epoch rewards (profit): -9.48e+01 after 61 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 62 steps\n",
      "Total epoch rewards (profit): -9.17e+01 after 63 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 64 steps\n",
      "Total epoch rewards (profit): -1.27e+03 after 65 steps\n",
      "Total epoch rewards (profit): 3.92e+02 after 66 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 67 steps\n",
      "Total epoch rewards (profit): -6.82e+02 after 68 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 69 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 70 steps\n",
      "Total epoch rewards (profit): -2.89e+03 after 71 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 72 steps\n",
      "Total epoch rewards (profit): 3.82e+02 after 73 steps\n",
      "Total epoch rewards (profit): 8.56e+02 after 74 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 75 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 76 steps\n",
      "Total epoch rewards (profit): -9.37e+02 after 77 steps\n",
      "Total epoch rewards (profit): 8.50e+02 after 78 steps\n",
      "Total epoch rewards (profit): -3.84e+02 after 79 steps\n",
      "Total epoch rewards (profit): -6.79e+01 after 80 steps\n",
      "Total epoch rewards (profit): -9.09e+01 after 81 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 82 steps\n",
      "Total epoch rewards (profit): -8.39e+00 after 83 steps\n",
      "Total epoch rewards (profit): 2.98e+02 after 84 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 85 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 86 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 87 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 88 steps\n",
      "Total epoch rewards (profit): -1.45e+00 after 89 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 90 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 91 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 92 steps\n",
      "Total epoch rewards (profit): -1.05e+03 after 93 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 94 steps\n",
      "Total epoch rewards (profit): 1.65e+03 after 95 steps\n",
      "Total epoch rewards (profit): 6.37e+02 after 96 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 97 steps\n",
      "Total epoch rewards (profit): -5.75e+02 after 98 steps\n",
      "Total epoch rewards (profit): 2.23e+03 after 99 steps\n",
      "Total epoch rewards (profit): 2.88e+01 after 100 steps\n",
      "Total epoch rewards (profit): 1.68e+02 after 101 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 102 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 103 steps\n",
      "Total epoch rewards (profit): 3.23e+02 after 104 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 105 steps\n",
      "Total epoch rewards (profit): 1.31e+02 after 106 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 107 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 108 steps\n",
      "Total epoch rewards (profit): 1.32e+03 after 109 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 110 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 111 steps\n",
      "Total epoch rewards (profit): 1.89e+01 after 112 steps\n",
      "Total epoch rewards (profit): 4.76e+02 after 113 steps\n",
      "Total epoch rewards (profit): 6.40e+00 after 114 steps\n",
      "Total epoch rewards (profit): -3.21e+02 after 115 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 116 steps\n",
      "Total epoch rewards (profit): 3.04e+03 after 117 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 118 steps\n",
      "Total epoch rewards (profit): -1.98e+03 after 119 steps\n",
      "Total epoch rewards (profit): 2.86e+02 after 120 steps\n",
      "Total epoch rewards (profit): -7.45e+01 after 121 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 122 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 123 steps\n",
      "Total epoch rewards (profit): -4.61e+01 after 124 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 125 steps\n",
      "Total epoch rewards (profit): -1.52e+02 after 126 steps\n",
      "Total epoch rewards (profit): 4.02e+03 after 127 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 128 steps\n",
      "Total epoch rewards (profit): 8.05e+02 after 129 steps\n",
      "Total epoch rewards (profit): -1.05e+03 after 130 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 131 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 132 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 133 steps\n",
      "Total epoch rewards (profit): 4.26e+02 after 134 steps\n",
      "Total epoch rewards (profit): 8.90e-01 after 135 steps\n",
      "Total epoch rewards (profit): 2.90e+00 after 136 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 137 steps\n",
      "Total epoch rewards (profit): -4.55e+02 after 138 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 139 steps\n",
      "Total epoch rewards (profit): -9.35e+02 after 140 steps\n",
      "Total epoch rewards (profit): 1.91e+02 after 141 steps\n",
      "Total epoch rewards (profit): 3.16e+02 after 142 steps\n",
      "Total epoch rewards (profit): -1.92e+00 after 143 steps\n",
      "Total epoch rewards (profit): 3.17e+03 after 144 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 145 steps\n",
      "Total epoch rewards (profit): -1.15e+02 after 146 steps\n",
      "Total epoch rewards (profit): -3.54e+02 after 147 steps\n",
      "Total epoch rewards (profit): -1.22e+02 after 148 steps\n",
      "Total epoch rewards (profit): -5.49e+03 after 149 steps\n",
      "Total epoch rewards (profit): -1.24e+03 after 150 steps\n",
      "Total epoch rewards (profit): 1.02e+02 after 151 steps\n",
      "Total epoch rewards (profit): -2.04e+02 after 152 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch rewards (profit): -2.81e+01 after 153 steps\n",
      "Total epoch rewards (profit): 1.09e+03 after 154 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 155 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 156 steps\n",
      "Total epoch rewards (profit): -4.67e+02 after 157 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 158 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 159 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 160 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 161 steps\n",
      "Total epoch rewards (profit): -5.73e+01 after 162 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 163 steps\n",
      "Total epoch rewards (profit): -5.01e+01 after 164 steps\n",
      "Total epoch rewards (profit): -3.42e+02 after 165 steps\n",
      "Total epoch rewards (profit): 9.40e-01 after 166 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 167 steps\n",
      "Total epoch rewards (profit): 9.55e+02 after 168 steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m     11\u001b[0m     total_training_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "File \u001b[0;32m~/Projects/CISC_856/Environment.py:205\u001b[0m, in \u001b[0;36mcryptoTrade.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_end_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_size\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Set the prices\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_price \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_index]\n",
      "File \u001b[0;32m~/Projects/CISC_856/Environment.py:190\u001b[0m, in \u001b[0;36mcryptoTrade.scale_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaled_data[column]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_index]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    189\u001b[0m     scaling_values[column] \u001b[38;5;241m=\u001b[39m scaled_data[column]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_index]\n\u001b[0;32m--> 190\u001b[0m     scaled_data[column] \u001b[38;5;241m=\u001b[39m scaled_data[column]\u001b[38;5;241m/\u001b[39mscaling_values[column]\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     scaling_values[column] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/frame.py:3845\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3842\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(existing_piece, DataFrame):\n\u001b[1;32m   3843\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m-> 3845\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis), key, value)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;66;03m# check if we are modifying a copy\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m \u001b[38;5;66;03m# try to set first as we want an invalid\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;66;03m# value exception to occur first\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/frame.py:3794\u001b[0m, in \u001b[0;36mDataFrame._iset_item_mgr\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iset_item_mgr\u001b[39m(\n\u001b[1;32m   3791\u001b[0m     \u001b[38;5;28mself\u001b[39m, loc: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray, value, inplace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3792\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;66;03m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[39;00m\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/internals/managers.py:1141\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   1139\u001b[0m             removed_blknos\u001b[38;5;241m.\u001b[39mappend(blkno_l)\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1141\u001b[0m             \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs[blk\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(blk))\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(removed_blknos):\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m# Remove blocks & update blknos accordingly\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/internals/blocks.py:388\u001b[0m, in \u001b[0;36mBlock.delete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mDelete given loc(-s) from block in-place.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Argument 1 to \"delete\" has incompatible type \"Union[ndarray[Any, Any],\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# ExtensionArray]\"; expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Sequence[_SupportsArray[dtype[Any]]], Sequence[Sequence\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# [_SupportsArray[dtype[Any]]]], Sequence[Sequence[Sequence[\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# _SupportsArray[dtype[Any]]]]], Sequence[Sequence[Sequence[Sequence[\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# _SupportsArray[dtype[Any]]]]]]]\"  [arg-type]\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmgr_locs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr_locs\u001b[38;5;241m.\u001b[39mdelete(loc)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/lib/function_base.py:5131\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   5128\u001b[0m         keep[obj,] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   5130\u001b[0m     slobj[axis] \u001b[38;5;241m=\u001b[39m keep\n\u001b[0;32m-> 5131\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   5133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m   5134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap(new)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The main loop\n",
    "epsilon, max_epsilon, min_epsilon = 1, 1, 0.01\n",
    "decay = 0.01\n",
    "\n",
    "memory = []\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "steps_to_update_target_model = 0\n",
    "for episode in range(1000):\n",
    "    total_training_rewards = 0\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        steps_to_update_target_model += 1\n",
    "\n",
    "        # Implement epsilon greedy learning\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = int(np.random.choice(len(env.action_space)))\n",
    "        else: \n",
    "            action = int(model.predict(convert_to_1d(observation, single=True)).argmax())\n",
    "            \n",
    "        # Now step the simulation\n",
    "        actual_action, new_observation, reward, done = env.step(action)\n",
    "        memory.append([observation.copy(), action, actual_action, reward, new_observation.copy(), done])\n",
    "        \n",
    "        # Update the neural network\n",
    "        if (steps_to_update_target_model % 4 == 0) or done:\n",
    "            X = train(env, memory, model, target_model, done)\n",
    "            \n",
    "        observation = new_observation\n",
    "        total_training_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            #print('Total epoch rewards (profit): {:.2e} after {} steps'.format(total_training_rewards, episode))\n",
    "            print('Total epoch rewards (profit): {:.2e} after {} steps'.format(reward, episode))\n",
    "\n",
    "            if steps_to_update_target_model >= 100:\n",
    "#                 print('Copying main network weights to the target network weights')\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                steps_to_update_target_model = 0\n",
    "            break\n",
    "        \n",
    "    # Update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "\n",
    "# target_model.save('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6642d0c",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc314310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the env and the model\n",
    "testing_data_path = \"data/testing_2022.df\"\n",
    "testing_env = cryptoTrade(testing_data_path, episode_size=720)\n",
    "\n",
    "observation = testing_env.reset()\n",
    "done = False\n",
    "val_memory = []\n",
    "\n",
    "while not done:\n",
    "    action = int(model.predict(convert_to_1d(observation, single=True)).argmax())\n",
    "    actual_action, new_observation, reward, done = testing_env.step(action)\n",
    "    \n",
    "    info = {\"observation\":observation.copy(), \"action\":action, \"actual_action\":actual_action, \n",
    "            \"reward\":reward, \"new_observation\":new_observation.copy(), \"done\":done}\n",
    "    val_memory.append(info)\n",
    "    \n",
    "    observation = new_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "observations = [x[\"observation\"] for x in val_memory]\n",
    "actions = [x[\"action\"] for x in val_memory]\n",
    "actual_actions = [x[\"actual_action\"] for x in val_memory]\n",
    "rewards = [x[\"reward\"] for x in val_memory]\n",
    "new_observations = [x[\"new_observation\"] for x in val_memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Profits over a random 12h interval\")\n",
    "plt.xlabel(\"Minutes\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPAND THE DATA WE ARE USING\n",
    "# TEST ON TESTING DATASET\n",
    "# PLOT THE RESULTS\n",
    "# CREATE BUY SELL ANIMATION\n",
    "# DO A DNN AFTER EVAL IS CREATED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
