{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40915ae3",
   "metadata": {},
   "source": [
    "## Main file for training and saving an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf061b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d5d5b",
   "metadata": {},
   "source": [
    "### Import the agent and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8746a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from Environment import cryptoTrade\n",
    "\n",
    "from networks.Deep_RL_agents import DNN_agent, convert_to_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b65548",
   "metadata": {},
   "source": [
    "### Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd049278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, memory, model, target_model, done):\n",
    "    learning_rate = 0.7\n",
    "    discount_factor = 0.9\n",
    "    \n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    if len(memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "    \n",
    "    batch_size = 64\n",
    "    mini_batch_indexes = np.random.choice(np.arange(len(memory)), size=batch_size, replace=False)\n",
    "\n",
    "    current_states = [memory[i][0] for i in mini_batch_indexes]\n",
    "    current_qs_list = model.predict(convert_to_1d(current_states, single=False))\n",
    "    \n",
    "    new_current_states = [memory[i][4] for i in mini_batch_indexes]\n",
    "    future_qs_list = target_model.predict(convert_to_1d(new_current_states, single=False))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, i in enumerate(mini_batch_indexes):\n",
    "        (observation, action, actual_action, reward, new_observation, done) = memory[i]\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "\n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "    \n",
    "    X = convert_to_1d(X, single=False)\n",
    "    return X, Y\n",
    "    print(X)\n",
    "    model.fit(X, np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ff177",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887bd785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:08:45.111569: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-07 16:08:45.111700: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Create the env and the model\n",
    "training_data_path = \"data/training_2015_2021.df\"\n",
    "env = cryptoTrade(training_data_path, episode_size=720)\n",
    "\n",
    "model = DNN_agent(env.observation_space, env.action_space)\n",
    "target_model = DNN_agent(env.observation_space, env.action_space)\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb27e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch rewards (profit): 0.00e+00 after 0 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:08:56.279465: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-07 16:08:56.346149: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-07 16:08:56.397838: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch rewards (profit): -9.10e+00 after 1 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:09:13.571535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch rewards (profit): 9.34e+01 after 2 steps\n",
      "Total epoch rewards (profit): -1.24e+02 after 3 steps\n",
      "Total epoch rewards (profit): -1.24e+02 after 4 steps\n",
      "Total epoch rewards (profit): 4.26e+01 after 5 steps\n",
      "Total epoch rewards (profit): -2.93e+02 after 6 steps\n",
      "Total epoch rewards (profit): 2.82e+02 after 7 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 8 steps\n",
      "Total epoch rewards (profit): -6.20e+02 after 9 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 10 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 11 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 12 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 13 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 14 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 15 steps\n",
      "Total epoch rewards (profit): -2.47e+01 after 16 steps\n",
      "Total epoch rewards (profit): 2.54e+02 after 17 steps\n",
      "Total epoch rewards (profit): 4.45e+01 after 18 steps\n",
      "Total epoch rewards (profit): -6.05e+01 after 19 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 20 steps\n",
      "Total epoch rewards (profit): 0.00e+00 after 21 steps\n"
     ]
    }
   ],
   "source": [
    "# The main loop\n",
    "epsilon, max_epsilon, min_epsilon = 1, 1, 0.01\n",
    "decay = 0.01\n",
    "\n",
    "memory = []\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "steps_to_update_target_model = 0\n",
    "for episode in range(1000):\n",
    "    total_training_rewards = 0\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        steps_to_update_target_model += 1\n",
    "\n",
    "        # Implement epsilon greedy learning\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = int(np.random.choice(len(env.action_space)))\n",
    "        else: \n",
    "            action = int(model.predict(convert_to_1d(observation, single=True)).argmax())\n",
    "            \n",
    "        # Now step the simulation\n",
    "        actual_action, new_observation, reward, done = env.step(action)\n",
    "        memory.append([observation.copy(), action, actual_action, reward, new_observation.copy(), done])\n",
    "        \n",
    "        # Update the neural network\n",
    "        if (steps_to_update_target_model % 4 == 0) or done:\n",
    "            X = train(env, memory, model, target_model, done)\n",
    "            \n",
    "        observation = new_observation\n",
    "        total_training_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            #print('Total epoch rewards (profit): {:.2e} after {} steps'.format(total_training_rewards, episode))\n",
    "            print('Total epoch rewards (profit): {:.2e} after {} steps'.format(reward, episode))\n",
    "\n",
    "            if steps_to_update_target_model >= 100:\n",
    "#                 print('Copying main network weights to the target network weights')\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                steps_to_update_target_model = 0\n",
    "            break\n",
    "        \n",
    "    # Update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "\n",
    "# target_model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN_agent(env.observation_space, env.action_space)\n",
    "model.predict(convert_to_1d(observation, single=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(convert_to_1d(observation, single=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663559a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.dim_2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749446e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory[-1][4][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913064b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory[-1][4][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7a39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18667155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e5fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1\n",
    "X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6642d0c",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc314310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the env and the model\n",
    "testing_data_path = \"data/testing_2022.df\"\n",
    "testing_env = cryptoTrade(testing_data_path, episode_size=720)\n",
    "\n",
    "observation = testing_env.reset()\n",
    "done = False\n",
    "val_memory = []\n",
    "\n",
    "while not done:\n",
    "    action = int(model.predict(convet_to_ragged_tensor(observation, single=True)).argmax())\n",
    "    actual_action, new_observation, reward, done = testing_env.step(action)\n",
    "    \n",
    "    info = {\"observation\":observation.copy(), \"action\":action, \"actual_action\":actual_action, \n",
    "            \"reward\":reward, \"new_observation\":new_observation.copy(), \"done\":done}\n",
    "    val_memory.append(info)\n",
    "    \n",
    "    observation = new_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "observations = [x[\"observation\"] for x in val_memory]\n",
    "actions = [x[\"action\"] for x in val_memory]\n",
    "actual_actions = [x[\"actual_action\"] for x in val_memory]\n",
    "rewards = [x[\"reward\"] for x in val_memory]\n",
    "new_observations = [x[\"new_observation\"] for x in val_memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Profits over a random 12h interval\")\n",
    "plt.xlabel(\"Minutes\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPAND THE DATA WE ARE USING\n",
    "# TEST ON TESTING DATASET\n",
    "# PLOT THE RESULTS\n",
    "# CREATE BUY SELL ANIMATION\n",
    "# DO A DNN AFTER EVAL IS CREATED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
