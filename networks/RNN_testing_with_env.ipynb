{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9673e235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from Environment import cryptoTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ef425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "def convet_to_ragged_tensor(obs, single=True):\n",
    "    # Make sure nesting depth is consistent\n",
    "    if single:\n",
    "        for i, value in enumerate(obs):\n",
    "            if not isinstance(value, list):\n",
    "                obs[i] = list([value])\n",
    "\n",
    "        return tf.ragged.constant([obs])\n",
    "\n",
    "    else:\n",
    "        for i, entry in enumerate(obs):\n",
    "            for j, value in enumerate(entry):\n",
    "                if not isinstance(value, list):\n",
    "                    obs[i][j] = list([value])\n",
    "\n",
    "        return tf.ragged.constant(obs)\n",
    "    \n",
    "init = tf.keras.initializers.he_uniform(seed=None)\n",
    "\n",
    "def agent(observation_space, action_space):\n",
    "    \n",
    "    # Convert input to a ragged tensor\n",
    "    observation_space_tensor = convet_to_ragged_tensor(observation_space)\n",
    "    \n",
    "    # Get maximum sequence length\n",
    "    max_seq = observation_space_tensor.bounding_shape()[-1]\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=[None, max_seq], dtype=tf.float32, ragged=True),\n",
    "        tf.keras.layers.LSTM(64, kernel_initializer=init),\n",
    "        tf.keras.layers.Dense(len(action_space), activation='linear', kernel_initializer=init)\n",
    "    ])\n",
    "    \n",
    "    # Can also use Huber loss?\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ea2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, memory, model, target_model, done):\n",
    "    learning_rate = 0.7\n",
    "    discount_factor = 0.9\n",
    "    \n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    if len(memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "    \n",
    "    batch_size = 64\n",
    "    mini_batch_indexes = np.random.choice(np.arange(len(memory)), size=batch_size, replace=False)\n",
    "\n",
    "    current_states = [memory[i][0] for i in mini_batch_indexes]\n",
    "    current_qs_list = model.predict(convet_to_ragged_tensor(current_states, single=False))\n",
    "    \n",
    "    new_current_states = [memory[i][3] for i in mini_batch_indexes]\n",
    "    future_qs_list = target_model.predict(convet_to_ragged_tensor(new_current_states, single=False))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, i in enumerate(mini_batch_indexes):\n",
    "        (observation, action, reward, new_observation, done) = memory[i]\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "\n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "    \n",
    "    X = convet_to_ragged_tensor(X, single=False)\n",
    "    model.fit(X, np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4bf0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:24:59.765347: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-04 19:24:59.765429: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'cryptoTrade' object has no attribute 'current_shares'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mpredict(convet_to_ragged_tensor(observation, single\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39margmax())\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Now step the simulation\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m new_observation, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m memory\u001b[38;5;241m.\u001b[39mappend([observation, action, reward, new_observation, done])\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Update the neural network\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/CISC_856/networks/../Environment.py:55\u001b[0m, in \u001b[0;36mcryptoTrade.step\u001b[0;34m(self, raw_action)\u001b[0m\n\u001b[1;32m     52\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_valid_action(action)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Buy/sell shares and calculate profit\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m profit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Update net worth\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_net_worth()\n",
      "File \u001b[0;32m~/Projects/CISC_856/networks/../Environment.py:74\u001b[0m, in \u001b[0;36mcryptoTrade.execute_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Have already checked that the given action is valid\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Will also return the reward (sell_price-buy_price)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_shares \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuying_power \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoday_price\u001b[38;5;241m*\u001b[39maction\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     78\u001b[0m         \n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m#### BUY ####\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'cryptoTrade' object has no attribute 'current_shares'"
     ]
    }
   ],
   "source": [
    "# The main loop\n",
    "\n",
    "data_path = \"../data/Coinbase_BTCUSD_dailydata.csv\"\n",
    "env = cryptoTrade(data_path)\n",
    "env.reset()\n",
    "\n",
    "epsilon, max_epsilon, min_epsilon = 1, 1, 0.01\n",
    "decay = 0.01\n",
    "\n",
    "model = agent(env.observation_space, env.action_space)\n",
    "target_model = agent(env.observation_space, env.action_space)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "memory = []\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "f = open(\"20220330_RNNTraining.txt\",\"w\")\n",
    "\n",
    "steps_to_update_target_model = 0\n",
    "for episode in range(300):\n",
    "    total_training_rewards = 0\n",
    "    \n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        steps_to_update_target_model += 1\n",
    "\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = int(np.random.choice(len(env.action_space)))\n",
    "        else: \n",
    "            # Choose the best action\n",
    "            action = int(model.predict(convet_to_ragged_tensor(observation, single=True)).argmax())\n",
    "            \n",
    "        # Now step the simulation\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        memory.append([observation, action, reward, new_observation, done])\n",
    "        \n",
    "        # Update the neural network\n",
    "        if steps_to_update_target_model % 4 ==0:#or done:\n",
    "            train(env, memory, model, target_model, done)\n",
    "            \n",
    "        #observation = new_observation\n",
    "        total_training_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))\n",
    "            #total_training_rewards += 1\n",
    "            txt = \"{:.2f}\\n\"\n",
    "            f.write(txt.format(total_training_rewards))\n",
    "\n",
    "            if steps_to_update_target_model >= 100:\n",
    "                print('Copying main network weights to the target network weights')\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                steps_to_update_target_model = 0\n",
    "            break\n",
    "        \n",
    "    # Update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "\n",
    "f.close()\n",
    "target_model.save('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0e7f2",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6574bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment to predict over\n",
    "observation = env.reset()\n",
    "done = False\n",
    "val_memory = []\n",
    "\n",
    "while not done:\n",
    "    action = int(model.predict(convet_to_ragged_tensor(observation, single=True)).argmax())\n",
    "    new_observation, reward, done = env.step(action)\n",
    "    val_memory.append([observation, action, reward, new_observation, done])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simulation to see what happened\n",
    "data = env.data\n",
    "actions = np.array(val_memory, dtype=object)[:, 1]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560dc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
